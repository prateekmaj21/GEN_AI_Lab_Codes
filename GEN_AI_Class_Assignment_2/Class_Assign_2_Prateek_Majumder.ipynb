{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class Participation: Return structured output with a ReAct style agent\n",
        "\n",
        "# Name: Prateek Majumder\n"
      ],
      "metadata": {
        "id": "Ozcv0EZ8LKvH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Oiyd9FfOLHX8"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain_anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install meteostat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udXCt5dMQP0h",
        "outputId": "16d847df-7ea4-457b-9615-fbbca0d2cb89"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting meteostat\n",
            "  Downloading meteostat-1.6.8-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.11/dist-packages (from meteostat) (2.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from meteostat) (2024.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from meteostat) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1->meteostat) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1->meteostat) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1->meteostat) (1.17.0)\n",
            "Downloading meteostat-1.6.8-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: meteostat\n",
            "Successfully installed meteostat-1.6.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the anthropic key from: https://www.merge.dev/blog/anthropic-api-key"
      ],
      "metadata": {
        "id": "M2U6xx8NLwWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6ftHUMZLgwD",
        "outputId": "b4c647bf-7cf3-4ac4-896d-d1a79b756f6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANTHROPIC_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "from langchain_core.tools import tool\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "\n",
        "class WeatherResponse(BaseModel):\n",
        "    \"\"\"Respond to the user with this\"\"\"\n",
        "\n",
        "    temperature: float = Field(description=\"The temperature in fahrenheit\")\n",
        "    wind_directon: str = Field(\n",
        "        description=\"The direction of the wind in abbreviated form\"\n",
        "    )\n",
        "    wind_speed: float = Field(description=\"The speed of the wind in km/h\")\n",
        "\n",
        "\n",
        "# Inherit 'messages' key from MessagesState, which is a list of chat messages\n",
        "class AgentState(MessagesState):\n",
        "    # Final structured response from the agent\n",
        "    final_response: WeatherResponse\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
        "    \"\"\"Use this to get weather information.\"\"\"\n",
        "    if city == \"nyc\":\n",
        "        return \"It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees\"\n",
        "    elif city == \"sf\":\n",
        "        return \"It is 75 degrees and sunny in SF, with 3 mph winds in the South-East direction\"\n",
        "    else:\n",
        "        raise AssertionError(\"Unknown city\")\n",
        "\n",
        "\n",
        "tools = [get_weather]\n",
        "\n",
        "model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
        "\n",
        "model_with_tools = model.bind_tools(tools)\n",
        "model_with_structured_output = model.with_structured_output(WeatherResponse)"
      ],
      "metadata": {
        "id": "w14rE-z0Lq8b"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: AgentState):\n",
        "    response = model_with_tools.invoke(state[\"messages\"])\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# Define the function that responds to the user\n",
        "def respond(state: AgentState):\n",
        "    # We call the model with structured output in order to return the same format to the user every time\n",
        "    # state['messages'][-2] is the last ToolMessage in the convo, which we convert to a HumanMessage for the model to use\n",
        "    # We could also pass the entire chat history, but this saves tokens since all we care to structure is the output of the tool\n",
        "    response = model_with_structured_output.invoke(\n",
        "        [HumanMessage(content=state[\"messages\"][-2].content)]\n",
        "    )\n",
        "    # We return the final answer\n",
        "    return {\"final_response\": response}\n",
        "\n",
        "\n",
        "# Define the function that determines whether to continue or not\n",
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    # If there is no function call, then we respond to the user\n",
        "    if not last_message.tool_calls:\n",
        "        return \"respond\"\n",
        "    # Otherwise if there is, we continue\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cnaopCotL69I"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"respond\", respond)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"respond\": \"respond\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "workflow.add_edge(\"respond\", END)\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "Br33Qx3MNAMc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage:**"
      ],
      "metadata": {
        "id": "QldjvnazNDFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
        "    \"final_response\"\n",
        "]"
      ],
      "metadata": {
        "id": "M4VtFkjQQimx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc3eonHWS7Fs",
        "outputId": "73135e76-5f11-4480-ad7a-ecaef952fd4d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(temperature=75.0, wind_directon='SE', wind_speed=3.0)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}